Layer (type)                 Output Shape              Param #
=================================================================
conv2d_13 (Conv2D)           (None, 48, 48, 8)         208
batch_normalization_7 (Batch (None, 48, 48, 8)         32
activation_7 (Activation)    (None, 48, 48, 8)         0
conv2d_14 (Conv2D)           (None, 48, 48, 8)         1608
batch_normalization_8 (Batch (None, 48, 48, 8)         32
activation_8 (Activation)    (None, 48, 48, 8)         0
max_pooling2d_7 (MaxPooling2 (None, 24, 24, 8)         0
dropout_7 (Dropout)          (None, 24, 24, 8)         0
conv2d_15 (Conv2D)           (None, 24, 24, 16)        3216
batch_normalization_9 (Batch (None, 24, 24, 16)        64
activation_9 (Activation)    (None, 24, 24, 16)        0
conv2d_16 (Conv2D)           (None, 24, 24, 16)        6416
batch_normalization_10 (Batc (None, 24, 24, 16)        64
activation_10 (Activation)   (None, 24, 24, 16)        0
max_pooling2d_8 (MaxPooling2 (None, 12, 12, 16)        0
dropout_8 (Dropout)          (None, 12, 12, 16)        0
conv2d_17 (Conv2D)           (None, 12, 12, 32)        12832
batch_normalization_11 (Batc (None, 12, 12, 32)        128
activation_11 (Activation)   (None, 12, 12, 32)        0
conv2d_18 (Conv2D)           (None, 12, 12, 32)        25632
batch_normalization_12 (Batc (None, 12, 12, 32)        128
activation_12 (Activation)   (None, 12, 12, 32)        0
max_pooling2d_9 (MaxPooling2 (None, 6, 6, 32)          0
dropout_9 (Dropout)          (None, 6, 6, 32)          0
flatten_3 (Flatten)          (None, 1152)              0
dense_5 (Dense)              (None, 128)               147584
dropout_10 (Dropout)         (None, 128)               0
dense_6 (Dense)              (None, 7)                 903
=================================================================
Total params: 198,847
Trainable params: 198,623
Non-trainable params: 224
_____________________________________________________________

filter_size=5*5
loss: 1.1506 - acc: 0.5614 - val_loss: 1.1772 - val_acc: 0.5536 (with class weight)
50 epoch
loss: 1.2115 - acc: 0.5363 - val_loss: 1.2443 - val_acc: 0.5336
loss: 1.0603 - acc: 0.6028 - val_loss: 1.1964 - val_acc: 0.5602
with weight initialization
kernel_initializer='he_uniform', bias_initializer='zeros',
kernel_regularizer=regularizers.l2(0.01), bias_regularizer=None
loss: 1.6022 - acc: 0.3982 - val_loss: 1.5388 - val_acc: 0.4373

filter_size=3*3
100 epoch
loss: 1.0954 - acc: 0.5857 - val_loss: 1.1029 - val_acc: 0.5832
loss: 1.0374 - acc: 0.6023 - val_loss: 1.1102 - val_acc: 0.5911


move batch_normalization after activation layer
=================================================================
conv2d_1 (Conv2D)            (None, 48, 48, 8)         80
activation_1 (Activation)    (None, 48, 48, 8)         0
batch_normalization_1 (Batch (None, 48, 48, 8)         32
conv2d_2 (Conv2D)            (None, 48, 48, 8)         584
activation_2 (Activation)    (None, 48, 48, 8)         0
batch_normalization_2 (Batch (None, 48, 48, 8)         32
max_pooling2d_1 (MaxPooling2 (None, 24, 24, 8)         0
dropout_1 (Dropout)          (None, 24, 24, 8)         0
conv2d_3 (Conv2D)            (None, 24, 24, 16)        1168
activation_3 (Activation)    (None, 24, 24, 16)        0
batch_normalization_3 (Batch (None, 24, 24, 16)        64
conv2d_4 (Conv2D)            (None, 24, 24, 16)        2320
activation_4 (Activation)    (None, 24, 24, 16)        0
batch_normalization_4 (Batch (None, 24, 24, 16)        64
max_pooling2d_2 (MaxPooling2 (None, 12, 12, 16)        0
dropout_2 (Dropout)          (None, 12, 12, 16)        0
conv2d_5 (Conv2D)            (None, 12, 12, 32)        4640
activation_5 (Activation)    (None, 12, 12, 32)        0
batch_normalization_5 (Batch (None, 12, 12, 32)        128
conv2d_6 (Conv2D)            (None, 12, 12, 32)        9248
activation_6 (Activation)    (None, 12, 12, 32)        0
batch_normalization_6 (Batch (None, 12, 12, 32)        128
max_pooling2d_3 (MaxPooling2 (None, 6, 6, 32)          0
dropout_3 (Dropout)          (None, 6, 6, 32)          0
flatten_1 (Flatten)          (None, 1152)              0
dense_1 (Dense)              (None, 128)               147584
dropout_4 (Dropout)          (None, 128)               0
dense_2 (Dense)              (None, 7)                 903
=================================================================
Total params: 166,975
Trainable params: 166,751
Non-trainable params: 224
_____________________________________________________________

* filter_size=3*3
    [naive, 50 epoch]
        all data,   loss: 1.0970 - acc: 0.5836 - val_loss: 1.1443 - val_acc: 0.5786 (nothing changed)
        split5,     loss: 1.1498 - acc: 0.5640 - val_loss: 1.2244 - val_acc: 0.5437
                    Loss: 1.25, Accuracy: 54.20%
        split4,     loss: 1.1911 - acc: 0.5482 - val_loss: 1.2244 - val_acc: 0.5327
                    Loss: 1.22, Accuracy: 54.30%
        split3,     loss: 1.1301 - acc: 0.5626 - val_loss: 1.1982 - val_acc: 0.5514
                    Loss:
        split2,     loss: 1.1855 - acc: 0.5505 - val_loss: 1.1991 - val_acc: 0.5491
                    Loss:
        split1,     loss: 1.1831 - acc: 0.5459 - val_loss: 1.1932 - val_acc: 0.5395
                    Loss:
    [he_normal + ReLU]
        converge a littel bit faster, no improvement (maybe has more chances to get lower cost)
    [dataAug: origin + rotation]
        split5, loss: 0.9256 - acc: 0.6464 - val_loss: 0.9674 - val_acc: 0.6510
                Loss: 1, Accuracy: 61.25%
        split4, loss: 0.9600 - acc: 0.6385 - val_loss: 1.0349 - val_acc: 0.6239
                Loss:
        split3, loss: 0.9607 - acc: 0.6407 - val_loss: 0.9837 - val_acc: 0.6314
                Loss:
        split2, loss: 1.0838 - acc: 0.5931 - val_loss: 1.0983 - val_acc: 0.5866
                Loss:
        split1, loss: 1.0452 - acc: 0.6049 - val_loss: 1.0505 - val_acc: 0.6014
                Loss:

    [dataAug: origin + shear]
        split5,     loss: 0.9531 - acc: 0.6386 - val_loss: 0.9076 - val_acc: 0.6649
                    Loss: 1.18, Accuracy: 56.57%
        split4,     loss: 0.8913 - acc: 0.6644 - val_loss: 0.9283 - val_acc: 0.6507
                    Loss:
        split3,     loss: 0.8679 - acc: 0.6739 - val_loss: 0.8799 - val_acc: 0.6709
                    Loss:
        split2,     loss: 0.8721 - acc: 0.6733 - val_loss: 0.9081 - val_acc: 0.6558
                    Loss:
        split1,     loss: 0.8296 - acc: 0.6873 - val_loss: 0.8678 - val_acc: 0.6756
                    Loss:
    [dataAug: origin + zoom]
        split5,     loss: 1.0291 - acc: 0.6100 - val_loss: 1.0587 - val_acc: 0.6000
                    Loss: 1.17, Accuracy: 55.91%
        split4,     loss: 1.0892 - acc: 0.5874 - val_loss: 1.1197 - val_acc: 0.5737
                    Loss:
        split3,     loss: 0.9872 - acc: 0.6265 - val_loss: 1.0564 - val_acc: 0.6017
                    Loss:
        split2,     loss: 1.0917 - acc: 0.5880 - val_loss: 1.1310 - val_acc: 0.5638
                    Loss:
        split1,     loss: 1.0286 - acc: 0.6091 - val_loss: 1.0666 - val_acc: 0.5916
                    Loss:
    [dataAug: origin + horizontal flip]
        split5,     loss: 1.0189 - acc: 0.6130 - val_loss: 1.0612 - val_acc: 0.6074
                    Loss: 1.16, Accuracy: 57.48%
        split4,     loss: 1.0000 - acc: 0.6187 - val_loss: 1.0816 - val_acc: 0.6024
                    Loss:
        split3,     loss: 1.0263 - acc: 0.6108 - val_loss: 1.0623 - val_acc: 0.6053
                    Loss:
        split2,     loss: 1.0444 - acc: 0.6072 - val_loss: 1.0840 - val_acc: 0.5966
                    Loss:
        split1,     loss: 1.0401 - acc: 0.6080 - val_loss: 1.0653 - val_acc: 0.5943
                    Loss:

